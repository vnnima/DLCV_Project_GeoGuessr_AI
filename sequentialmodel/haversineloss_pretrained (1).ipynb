{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ot7HwUXrbvLP"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# folder with pictures\n",
    "ROOT_DIR = r'C:\\Users\\Shadow\\Pictures\\Geogussr\\Projekt'\n",
    "\n",
    "# dir to csv files\n",
    "dir = r\"C:\\Users\\Shadow\\Documents\\sequentialmodel\\preprocess\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "N6AFTTy1w6-f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "#from torchsummary import summary\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "# Dataset\n",
    "from GeoGuessrDataset import GeoGuessrDataset\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tnrange\n",
    "import time\n",
    "import copy\n",
    "import pygeohash as phg\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable autoreloading of imported modules.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 471,
     "status": "ok",
     "timestamp": 1672146399703,
     "user": {
      "displayName": "Valdrin N.",
      "userId": "09743048349439752920"
     },
     "user_tz": -60
    },
    "id": "kj2xe7ZMKAs1",
    "outputId": "c81c3968-51d1-486d-f131-b8a171b62bf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check GPU support on your machine.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "HEIGHT = 512\n",
    "WIDTH = 2560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1100,
     "status": "ok",
     "timestamp": 1672147666496,
     "user": {
      "displayName": "Valdrin N.",
      "userId": "09743048349439752920"
     },
     "user_tz": -60
    },
    "id": "YZytNzAjKDPf",
    "outputId": "8f6445df-cf8e-4b70-b414-11a1a81a29fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.models.resnet import resnet50, ResNet50_Weights, resnet18, ResNet18_Weights\n",
    "\n",
    "\n",
    "# New weights with accuracy 80.858%\n",
    "resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "# Best available weights (currently alias for IMAGENET1K_V2)\n",
    "# Note that these weights may change across versions\n",
    "resnet50(weights=ResNet50_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 425,
     "status": "ok",
     "timestamp": 1672147548946,
     "user": {
      "displayName": "Valdrin N.",
      "userId": "09743048349439752920"
     },
     "user_tz": -60
    },
    "id": "Ob0MqD7HjcOD",
    "outputId": "550af3e4-a0bd-4353-d812-5f8d04873fe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of geohashes with samples 3139\n",
      "['South America' 'Asia' 'Africa' 'Oceania' 'North America' 'Europe'\n",
      " 'Antarctica']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shadow\\AppData\\Local\\Temp\\ipykernel_14532\\4247964589.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1_country[\"conti_code\"] = df1_country.apply(lambda continent: conti_map[continent[\"continent\"]], axis=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(dir+\"\\coordinates.csv\", delimiter=',', skiprows=0, low_memory=False)\n",
    "\n",
    "# We want a geohash precsion of 3 so that we get approximately 32768 cells, which will represent our classes.\n",
    "df['geohash']=df.apply(lambda coords: phg.encode(coords.latitude, coords.longitude, precision=3), axis=1)\n",
    "\n",
    "\n",
    "def geohash_to_decimal(geohash):\n",
    "    base_32 = '0123456789bcdefghjkmnpqrstuvwxyz';\n",
    "    geohash = geohash.lower()\n",
    "    # Check that the input geohash is a valid geohash\n",
    "    if not all(c in base_32 for c in geohash):\n",
    "        raise ValueError('Invalid geohash')\n",
    "    return sum([32**idx * base_32.index(char) for idx, char in enumerate(geohash[::-1])])\n",
    "\n",
    "df['geohash_decimal']=df.apply(lambda x: geohash_to_decimal(x[\"geohash\"]) ,axis=1)\n",
    "\n",
    "geohashes_with_samples = df[\"geohash_decimal\"].unique()\n",
    "print(\"Number of geohashes with samples\", len(geohashes_with_samples))\n",
    "\n",
    "geohash_map = { geo: i for i, geo in enumerate(geohashes_with_samples)}\n",
    "\n",
    "df[\"geo_code\"] = df.apply(lambda geohash: geohash_map[geohash[\"geohash_decimal\"]], axis=1)\n",
    "\n",
    "df[[\"filename\", \"latitude\",\"longitude\", \"geohash_decimal\", \"geo_code\"]].to_csv(dir+\"\\coordinates2.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add geohash center coordinates to csv\n",
    "\n",
    "def geohash_center(geohash_name):\n",
    "\n",
    "    # Decode the geohash to get the center coordinates and errors.\n",
    "    latitude, longitude, latitude_error, longitude_error = phg.decode_exactly(geohash_name)\n",
    "    return [latitude, longitude]\n",
    "\n",
    "df[\"geo_lat\"], df[\"geo_lon\"] = df.apply(lambda x: geohash_center(x[\"geohash\"])[0] ,axis=1), df.apply(lambda x: geohash_center(x[\"geohash\"])[1] ,axis=1)\n",
    "\n",
    "# drop the duplicates\n",
    "df = df.drop(columns=[\"filename\", \"latitude\", \"longitude\",\"geohash_decimal\",\"geohash\"] )\n",
    "df = df.drop_duplicates()\n",
    "array = df.to_numpy()\n",
    "array = np.array(array, dtype=np.float64)\n",
    "tensor = torch.tensor(array)\n",
    "torch.save(tensor, dir+'\\\\tensor.pt')\n",
    "# Save the DataFrame to a CSV file.\n",
    "df[[\"geo_code\", \"geo_lat\", \"geo_lon\"]].to_csv(dir+\"\\coords_center.csv\", index=False)\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(dir+\"\\coordinatestim.csv\", delimiter=',', skiprows=0, low_memory=False)\n",
    "df2 = pd.read_csv(dir+\"\\coordinates2.csv\", delimiter=',', skiprows=0, low_memory=False)\n",
    "\n",
    "df1_country = df1[['continent']]\n",
    "\n",
    "contis = df1_country['continent'].unique()\n",
    "\n",
    "\n",
    "conti_map = { conti: i for i, conti in enumerate(contis)}\n",
    "print(contis)\n",
    "df1_country[\"conti_code\"] = df1_country.apply(lambda continent: conti_map[continent[\"continent\"]], axis=1)\n",
    "\n",
    "df3 = pd.concat([df2,df1_country], axis=1)\n",
    "df3.to_csv(dir+\"\\coordinates3.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(dir+\"\\coordinates3.csv\", delimiter=',', skiprows=0, low_memory=False)\n",
    "\n",
    "# generate the continent names as keys\n",
    "keys = (df[\"continent\"].unique())\n",
    "\n",
    "\n",
    "\n",
    "# Add keys and all respective indices that contain a sample from that key to dict\n",
    "indices = {key: df.index[df[\"continent\"] == key].tolist() for key in keys}\n",
    "\n",
    "print(indices[\"South America\"][3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 889,
     "status": "ok",
     "timestamp": 1672147587577,
     "user": {
      "displayName": "Valdrin N.",
      "userId": "09743048349439752920"
     },
     "user_tz": -60
    },
    "id": "bE4hyeKWKELu",
    "outputId": "4309c3f8-60cc-4217-b5a5-0d1dffee53c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18818\n",
      "torch.Size([16, 3, 250, 1000]) torch.Size([16, 3139]) torch.Size([16, 2])\n"
     ]
    }
   ],
   "source": [
    "# Define the data transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # convert images to tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # normalize images\n",
    "    transforms.Resize((250, 1000))\n",
    "])\n",
    "\n",
    "# Load the dataset and split it into training and validation sets\n",
    "dataset = GeoGuessrDataset(csv_file=dir+'\\coordinates3.csv',\n",
    "                                    root_dir=ROOT_DIR, transform=transform, num_classes=7, indices=indices[\"Asia\"])\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "print(len(train_dataset))\n",
    "# Define the dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=5)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=5)\n",
    "\n",
    "dataloaders = {\"train\" : train_dataloader, \"val\": val_dataloader}\n",
    "dataset_sizes = {\"train\": train_size, \"val\" : val_size}\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    x, y, z = batch[\"image\"], batch[\"geohash\"], batch['gt']\n",
    "    print(x.shape, y.shape, z.shape)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3yX0Ph7LKEO7"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    best_acc = 0.0\n",
    "     \n",
    "    val_acc_history = []\n",
    "    train_acc_history = []\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    epoch_dist = []\n",
    "    \n",
    "    ######################\n",
    "    # single batch test\n",
    "    #batch=next(iter(dataloaders[\"train\"]))\n",
    "    \n",
    "    ##############################\n",
    "    for epoch in (pbar := tnrange(num_epochs)):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            running_dist = 0.0\n",
    "            # Iterate over data.\n",
    "            for idx, batch in enumerate(dataloaders[phase]):\n",
    "                inputs, labels  = batch[\"image\"], batch[\"geohash\"]\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "\n",
    "                    ##########################################################\n",
    "                    # loss switch :\n",
    "                    # first line Haversine + CE\n",
    "                    #second line benchmark\n",
    "                    # third line cross entropy\n",
    "                    #either use line 1 or 3\n",
    "                    #loss = criterion(outputs, gt, labels)\n",
    "\n",
    "                    loss = criterion2(outputs, labels.float())\n",
    "\n",
    "                    ##########################################################\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "\n",
    "                        optimizer.step()\n",
    "                _,labels = torch.max(labels, 1)\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                #running_dist += criterion1(outputs, gt, labels).item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            \n",
    "            ################################################\n",
    "            #adjust for single batch testing\n",
    "            #epoch_distance = running_dist / dataset_sizes[phase]\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            ###############################################################\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Dist: ')\n",
    "\n",
    "            \n",
    "                \n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                val_loss_history.append(epoch_loss)\n",
    "            else:\n",
    "                train_acc_history.append(epoch_acc)\n",
    "                train_loss_history.append(epoch_loss)\n",
    "                \n",
    "             # deep copy the model\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                PATH3 = r\"C:\\Users\\Shadow\\Documents\\DLCV_Project_GeoGuessr_AI-Basti\\models\\pretrainedresnet50_14epoch_Asia.tar\"\n",
    "                torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'val_loss_history': val_loss_history,\n",
    "                'val_acc_history': val_acc_history,\n",
    "                'train_loss_history' : train_loss_history,\n",
    "                'train_acc_history' : train_acc_history\n",
    "                }, PATH3)\n",
    "\n",
    "        print()\n",
    "\n",
    "    \n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def haversine_loss(lons1, lats1, lons2, lats2):\n",
    "    \"\"\"\n",
    "    Calculate the Haversine distance between two sets of longitudes and latitudes.\n",
    "\n",
    "    Parameters:\n",
    "        - lons1 (torch.tensor): Tensor of longitudes for the first set of points.\n",
    "        - lats1 (torch.tensor): Tensor of latitudes for the first set of points.\n",
    "        - lons2 (torch.tensor): Tensor of longitudes for the second set of points.\n",
    "        - lats2 (torch.tensor): Tensor of latitudes\n",
    "\n",
    "    Returns:\n",
    "        - distances (torch.tensor): Tensor of Haversine distances.\n",
    "    \"\"\"\n",
    "    # Convert longitudes and latitudes to radians.\n",
    "    lons1 = lons1.to(torch.float) * math.pi / 180\n",
    "    lats1 = lats1.to(torch.float) * math.pi / 180\n",
    "    lons2 = lons2.to(torch.float) * math.pi / 180\n",
    "    lats2 = lats2.to(torch.float) * math.pi / 180\n",
    "\n",
    "    # Calculate differences in longitudes and latitudes.\n",
    "    dlons = lons2 - lons1\n",
    "    dlats = lats2 - lats1\n",
    "\n",
    "    # Calculate intermediate values.\n",
    "    a = (torch.sin(dlats / 2)**2) + (torch.cos(lats1) * torch.cos(lats2) * (torch.sin(dlons / 2)**2))\n",
    "    c = 2 * torch.atan2(torch.sqrt(a), torch.sqrt(1 - a))\n",
    "\n",
    "    # Calculate distances.\n",
    "    R = 6371  # Earth radius in kilometers\n",
    "    distances = R * c\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# custom loss that transforms the predicted cluster and the ground truth label into coordinates\n",
    "# and then computes the haversine distance between them as loss combined with cross entropy loss\n",
    "\n",
    "\n",
    "class HaversineLoss_CE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HaversineLoss_CE, self).__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, output, gt, labels):\n",
    "        \"\"\"\n",
    "            output: (batchsize, num_clusters) Tensor with probabillities\n",
    "            gt: tupel contains groundtruth lat, lon\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        tensor = torch.load(dir+'\\\\tensor.pt')\n",
    "        # Extract the latitude and longitude from the predicted location.\n",
    "        #lat_cluster, lon_cluster = torch.zeros(len(output)), torch.zeros(len(output))\n",
    "        lat_cluster, lon_cluster = tensor[:,1], tensor[:,2]\n",
    "\n",
    "        # Calculate the Haversine distance between the predicted and target locations.\n",
    "        gt0 = torch.tensor(gt[:,0])\n",
    "        gt1 = torch.tensor(gt[:,1])\n",
    "        \n",
    "        distance = haversine_loss(lat_cluster, lon_cluster, gt0[:,None], gt1[:,None])\n",
    "        #print(torch.argmin(distance,dim=1))    \n",
    "        \n",
    "        \n",
    "        output1 = F.softmax(output, dim=1)\n",
    "        \n",
    "        # Return the loss.\n",
    "        distance = distance.to(device)\n",
    "        #print(torch.sum(distance*output,dim=1))\n",
    "        \n",
    "        loss = torch.mean(torch.sum(distance*output1,dim=1)) + 10000 * self.criterion(output, labels.float())\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used as another benchmark value\n",
    "\n",
    "\n",
    "\n",
    "class HaversineLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HaversineLoss, self).__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, output, gt, labels):\n",
    "        \"\"\"\n",
    "            output: (batchsize, num_clusters) Tensor with probabillities\n",
    "            gt: tupel contains groundtruth lat, lon\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        tensor = torch.load(dir+'\\\\tensor.pt')\n",
    "        # Extract the latitude and longitude from the predicted location.\n",
    "        #lat_cluster, lon_cluster = torch.zeros(len(output)), torch.zeros(len(output))\n",
    "        lat_cluster, lon_cluster = tensor[:,1], tensor[:,2]\n",
    "\n",
    "        # Calculate the Haversine distance between the predicted and target locations.\n",
    "        gt0 = torch.tensor(gt[:,0])\n",
    "        gt1 = torch.tensor(gt[:,1])\n",
    "        \n",
    "        distance = haversine_loss(lat_cluster, lon_cluster, gt0[:,None], gt1[:,None])\n",
    "        #print(torch.argmin(distance,dim=1))    \n",
    "        \n",
    "        \n",
    "        output1 = F.softmax(output, dim=1)\n",
    "        \n",
    "        # Return the loss.\n",
    "        distance = distance.to(device)\n",
    "        #print(torch.sum(distance*output,dim=1))\n",
    "        \n",
    "        loss = torch.mean(torch.sum(distance*output1,dim=1)) #+ 10000 * self.criterion(output, labels.float())\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IVLjMMowKER4"
   },
   "outputs": [],
   "source": [
    "model_ft = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "# model_ft = models.resnet18()\n",
    "\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, 3139)\n",
    "\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "\n",
    " \n",
    "criterion = HaversineLoss_CE()\n",
    "criterion1 = HaversineLoss()\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = torch.optim.Adam(model_ft.parameters(), lr=0.0001)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=6, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4792103,
     "status": "ok",
     "timestamp": 1672152510104,
     "user": {
      "displayName": "Valdrin N.",
      "userId": "09743048349439752920"
     },
     "user_tz": -60
    },
    "id": "uPdTudKxnSoo",
    "outputId": "7f192f3b-35a2-495d-8033-6a2712be1716",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4963ff33a2344e1f953a1cab1b0165fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 5.4869 Acc: 0.1105 Dist: \n",
      "val Loss: 4.6957 Acc: 0.1658 Dist: \n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 4.2169 Acc: 0.1942 Dist: \n",
      "val Loss: 4.2033 Acc: 0.2138 Dist: \n",
      "\n",
      "Epoch 3/14\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1672146064864,
     "user": {
      "displayName": "Valdrin N.",
      "userId": "09743048349439752920"
     },
     "user_tz": -60
    },
    "id": "cJdia4OvrAuy",
    "outputId": "5bc919cf-d916-482b-ec9a-77d089dcdfb9"
   },
   "outputs": [],
   "source": [
    "model_ft = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "# model_ft = models.resnet18()\n",
    "\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, 7)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7GEvmhLTrCA6"
   },
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = torch.load(r\"C:\\Users\\Shadow\\Documents\\DLCV_Project_GeoGuessr_AI-Basti\\models\\pretrainedresnet50_14epoch_Celoss.tar\")\n",
    "model_ft.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer_ft.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygeohash\n",
    "def decimal_to_geohash(decimal):\n",
    "    base_32 = '0123456789bcdefghjkmnpqrstuvwxyz'\n",
    "    geohash = ''\n",
    "    while decimal > 0:\n",
    "        geohash += base_32[decimal % 32]\n",
    "        decimal //= 32\n",
    "    return geohash[::-1]\n",
    "\n",
    "def geohash_to_lat_lon(geohash):\n",
    "    return pygeohash.decode(geohash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine import haversine\n",
    "\n",
    "def geogussr_score(model):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    geogussrscore = []\n",
    "    labelsaccum = []\n",
    "    predaccum0 = []\n",
    "    predaccum1 = []\n",
    "    predaccum2 = []\n",
    "    predaccum3 = []\n",
    "    predaccum4 = []\n",
    "    predaccum5 = []\n",
    "    diff = []\n",
    "    # Iterate over data.\n",
    "    for idx, batch in enumerate(dataloaders['val']):\n",
    "        inputs, labels = batch[\"image\"], batch[\"conti\"]\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        _,labels = torch.max(labels, 1)\n",
    "\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels)\n",
    "\n",
    "        #preds = geohashes_with_samples[preds.cpu()]\n",
    "        #labels = geohashes_with_samples[labels.cpu()]\n",
    "\n",
    "        #distance = []\n",
    "        #if len(inputs) == 1:\n",
    "         #   distance.append(haversine(geohash_to_lat_lon(decimal_to_geohash(preds)),geohash_to_lat_lon(decimal_to_geohash(labels))))\n",
    "        #else:\n",
    "         #   for i in range(len(inputs)):\n",
    "         #       distance.append(haversine(geohash_to_lat_lon(decimal_to_geohash(preds[i])),geohash_to_lat_lon(decimal_to_geohash(labels[i]))))\n",
    "        \n",
    "        if labels.cpu().numpy() == 0 and labels.cpu().numpy() != preds.cpu().numpy():\n",
    "            predaccum0.append(preds.cpu().numpy())\n",
    "        elif labels.cpu().numpy() == 1 and labels.cpu().numpy() != preds.cpu().numpy():\n",
    "            predaccum1.append(preds.cpu().numpy())\n",
    "        elif labels.cpu().numpy() == 2 and labels.cpu().numpy() != preds.cpu().numpy():    \n",
    "            predaccum2.append(preds.cpu().numpy())\n",
    "        elif labels.cpu().numpy() == 3 and labels.cpu().numpy() != preds.cpu().numpy():\n",
    "            predaccum3.append(preds.cpu().numpy())\n",
    "        elif labels.cpu().numpy() == 4 and labels.cpu().numpy() != preds.cpu().numpy():\n",
    "            predaccum4.append(preds.cpu().numpy())\n",
    "        elif labels.cpu().numpy() == 5 and labels.cpu().numpy() != preds.cpu().numpy():\n",
    "            predaccum5.append(preds.cpu().numpy())\n",
    "            \n",
    "    South_america = np.array(predaccum0)\n",
    "    Asia = np.array(predaccum1)\n",
    "    Africa = np.array(predaccum2)\n",
    "    Oceania = np.array(predaccum3)\n",
    "    NorthAmerica = np.array(predaccum4)\n",
    "    Europe = np.array(predaccum5)\n",
    "    \n",
    "    \n",
    "    South_america = South_america.ravel()\n",
    "    Asia = Asia.ravel()\n",
    "    Africa = Africa.ravel()\n",
    "    Oceania = Oceania.ravel()\n",
    "    NorthAmerica = NorthAmerica.ravel()\n",
    "    Europe = Europe.ravel()\n",
    "    \n",
    "    South_america = np.bincount(South_america,minlength=7)\n",
    "    Asia = np.bincount(Asia,minlength=7)\n",
    "    Africa = np.bincount(Africa,minlength=7)\n",
    "    Oceania = np.bincount(Oceania,minlength=7)\n",
    "    NorthAmerica = np.bincount(NorthAmerica,minlength=7)\n",
    "    Europe = np.bincount(Europe,minlength=7)\n",
    "    #x = x.reshape(x.shape[0]*x.shape[1]) \n",
    "    #y = y.reshape(y.shape[0]*y.shape[1]) \n",
    "    \n",
    "    \n",
    "    #x = np.bincount(x,minlength=7)\n",
    "    #y = np.bincount(y,minlength=7)\n",
    "    #diff = x-y\n",
    "    # Get the range of values in the data\n",
    "    values = ['South America' ,'Asia', 'Africa', 'Oceania', 'North America' ,'Europe',\n",
    " 'Antarctica'] #np.arange(len(South_america))\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    # Create the histogram using matplotlib\n",
    "    \n",
    "    plt.bar(values, South_america)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    # Add labels and show the plot\n",
    "    plt.suptitle('South_america', fontsize=16)\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.bar(values, Asia)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.suptitle('Asia', fontsize=16)\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.bar(values, Africa)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.suptitle('Africa', fontsize=16)\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.bar(values, Oceania)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.suptitle('Oceania', fontsize=16)\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.bar(values, NorthAmerica)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.suptitle('North America', fontsize=16)\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.bar(values, Europe)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.suptitle('Europe', fontsize=16)\n",
    "    plt.show()\n",
    "            \n",
    "        \n",
    "        #geogussrscore.append((5000*np.exp(-np.array(distance)/2000)).mean())\n",
    "        \n",
    "        \n",
    "\n",
    "    epoch_loss = running_loss / dataset_sizes['val']\n",
    "    epoch_acc = running_corrects.double() / dataset_sizes['val']\n",
    "    average_score = np.array(geogussrscore).mean()\n",
    "    print(f'Val Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Score: {average_score:.4f}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "geogussr_score(model_ft)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMUaWHSQQoRMwe+LN/u09r4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b6b26721452f2d1ef6e818f0649f3a5fe6d15e71e413fd5aa88ad7acf81094b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
