{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tnrange\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "\n",
    "import GeoImageDataset   # USE YOUR OWN\n",
    "import GeoGuessrLoss\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "torch.cuda.get_device_name(device=None)\n",
    "\n",
    "\n",
    "from GeoGuessrDataset import GeoGuessrDataset\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable autoreloading of imported modules.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset and (if needed) CSV-file for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nfull_dataloader = None\\n\\n# OVERWRITE OR DELETE MY \"location_data.csv\" and \"GeoImageDataset.py\"\\n# EXAMPLE WITH MY DATASET. DELETE AND REPLACE WITH YOURS\\nimage_paths = [\\n    \"..\\\\..\\\\..\\\\geogussr1\",\\n    \"..\\\\..\\\\..\\\\geogussr2\",\\n    \"..\\\\..\\\\..\\\\geogussr3\",\\n    \"..\\\\..\\\\..\\\\geogussr4\",\\n    \"..\\\\..\\\\..\\\\geogussr5\",\\n    \"..\\\\..\\\\..\\\\geogussr6\",\\n    \"..\\\\..\\\\..\\\\geogussr7\"\\n]\\neval_data_path = \"location_data.csv\"\\n\\nCSV_needed = True\\n# Create full CSV-file\\ndef create_CSV():\\n    with open(eval_data_path, \\'w\\', newline=\\'\\') as csvfile:  # Create/open file\\n        wr = csv.writer(csvfile)\\n        for path in image_paths:\\n            for file in os.listdir(os.fsencode(path)):\\n                filename = os.fsdecode(file)\\n                wr.writerow([path + \"\\\\\" + filename] + [float(x) for x in filename[4:-4].split(\",\")])\\n\\n# Define  dataset\\nif(CSV_needed):\\n    create_CSV()\\n\\nfull_dataset = GeoImageDataset.GeoImageDataset(\"\", eval_data_path) \\nprint(\"Dataset: \" ,  len(full_dataset))\\n\\nfull_dataloader = DataLoader(full_dataset, batch_size=8,\\n                    shuffle=True, num_workers=3)\\n# ------------------------------------------------------------------\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "full_dataloader = None\n",
    "\n",
    "# OVERWRITE OR DELETE MY \"location_data.csv\" and \"GeoImageDataset.py\"\n",
    "# EXAMPLE WITH MY DATASET. DELETE AND REPLACE WITH YOURS\n",
    "image_paths = [\n",
    "    \"..\\\\..\\\\..\\\\geogussr1\",\n",
    "    \"..\\\\..\\\\..\\\\geogussr2\",\n",
    "    \"..\\\\..\\\\..\\\\geogussr3\",\n",
    "    \"..\\\\..\\\\..\\\\geogussr4\",\n",
    "    \"..\\\\..\\\\..\\\\geogussr5\",\n",
    "    \"..\\\\..\\\\..\\\\geogussr6\",\n",
    "    \"..\\\\..\\\\..\\\\geogussr7\"\n",
    "]\n",
    "eval_data_path = \"location_data.csv\"\n",
    "\n",
    "CSV_needed = True\n",
    "# Create full CSV-file\n",
    "def create_CSV():\n",
    "    with open(eval_data_path, 'w', newline='') as csvfile:  # Create/open file\n",
    "        wr = csv.writer(csvfile)\n",
    "        for path in image_paths:\n",
    "            for file in os.listdir(os.fsencode(path)):\n",
    "                filename = os.fsdecode(file)\n",
    "                wr.writerow([path + \"\\\\\" + filename] + [float(x) for x in filename[4:-4].split(\",\")])\n",
    "\n",
    "# Define  dataset\n",
    "if(CSV_needed):\n",
    "    create_CSV()\n",
    "\n",
    "full_dataset = GeoImageDataset.GeoImageDataset(\"\", eval_data_path) \n",
    "print(\"Dataset: \" ,  len(full_dataset))\n",
    "\n",
    "full_dataloader = DataLoader(full_dataset, batch_size=8,\n",
    "                    shuffle=True, num_workers=3)\n",
    "# ------------------------------------------------------------------\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# folder with pictures\n",
    "ROOT_DIR = r'C:\\Users\\Shadow\\Pictures\\Geogussr\\Projekt'\n",
    "\n",
    "# dir to csv files\n",
    "dir = r\"C:\\Users\\Shadow\\Documents\\sequentialmodel\\preprocess\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103009\n"
     ]
    }
   ],
   "source": [
    "# Define the data transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # convert images to tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # normalize images\n",
    "    transforms.Resize((250, 1000))\n",
    "])\n",
    "\n",
    "# Load the dataset and split it into training and validation sets (fc_size is generated in the kernel above)\n",
    "dataset = GeoGuessrDataset(csv_file=dir+'\\coordinates3.csv',\n",
    "                                    root_dir=ROOT_DIR, transform=transform, num_classes=4000, indices=None)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size],generator=torch.Generator(\n",
    ").manual_seed(42))\n",
    "print(len(train_dataset))\n",
    "# Define the dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=6)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=6)\n",
    "\n",
    "dataloaders = {\"train\" : train_dataloader, \"val\": val_dataloader}\n",
    "dataset_sizes = {\"train\": train_size, \"val\" : val_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import resnet50, ResNet50_Weights, resnet18, ResNet18_Weights\n",
    "# Create model and change the last layer to output only 2 numbers\n",
    "model = torchvision.models.resnet18(weights=ResNet18_Weights.DEFAULT).cuda()\n",
    "model.fc = nn.Linear(512, 2).cuda()\n",
    "model = model.float()\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_criterion = GeoGuessrLoss.GeoGuessrLoss()\n",
    "loss_criterion = loss_criterion.float()\n",
    "loss_criterion = loss_criterion.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for training\n",
    "model_parameter_path = './GeoResNet_1.pth'  # Where the model parameters get loaded from/ saved to\n",
    "output_frequency = 100  # How often (in batches) the training function should make a print\n",
    "\n",
    "def train_all_data(epoches = 1):\n",
    "    # Load model\n",
    "    model.load_state_dict(torch.load(model_parameter_path))\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    for epoch in (pbar := tnrange(epoches)):  # loop over the dataset multiple times\n",
    "        print(f'Epoch {epoch+1}/{epoches}')\n",
    "        print('-' * 10)# Each epoch has a training and validation phase\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            \n",
    "            \n",
    "            for idx, batch in enumerate(dataloaders[phase]):\n",
    "                inputs, labels, gt  = batch[\"image\"], batch[\"geohash\"], batch['gt']\n",
    "                inputs = inputs.to(device)\n",
    "                labels = gt.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # forward + backward + optimize\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    \n",
    "                    \"\"\"\n",
    "                    # print statistics\n",
    "                    if idx % output_frequency == output_frequency-1:\n",
    "                        # Print time, example label and guesses and average loss\n",
    "                        named_tuple = time.localtime()\n",
    "                        time_string = time.strftime(\"%m/%d/%Y, %H:%M:%S\", named_tuple)\n",
    "                        print(time_string, \"\\t Label\", labels[0], labels[1])\n",
    "                        print(\"Output\", outputs[0], outputs[1])\n",
    "                        print(f'[{epoch + 1}, {idx + 1:5d}] loss: {running_loss / output_frequency:.3f}')\n",
    "                        running_loss = 0.0\n",
    "                    \"\"\"\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}')\n",
    "                \n",
    "        # Save model\n",
    "        torch.save(model.state_dict(), model_parameter_path)\n",
    "    PATH3 = r\"C:\\Users\\Shadow\\Documents\\markus\\losshist\\pretrainedresnet18_10epoch_contCNN_stats.tar\"\n",
    "    torch.save({\n",
    "    'train_loss_history' : train_loss_history,\n",
    "    'val_loss_history' : val_loss_history\n",
    "    }, PATH3)\n",
    "    torch.save(model.state_dict(), model_parameter_path)\n",
    "    print('Finished Training')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e79b53dcda4fb99702b95d5e437381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "----------\n",
      "train Loss: 1353.4711\n",
      "val Loss: 2526.3673\n",
      "Epoch 2/5\n",
      "----------\n",
      "train Loss: 1274.9240\n",
      "val Loss: 2401.7265\n",
      "Epoch 3/5\n",
      "----------\n",
      "train Loss: 1204.8605\n",
      "val Loss: 2384.6137\n",
      "Epoch 4/5\n",
      "----------\n",
      "train Loss: 1143.6093\n",
      "val Loss: 2452.8270\n",
      "Epoch 5/5\n",
      "----------\n",
      "train Loss: 1088.4556\n",
      "val Loss: 2531.8559\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "epoques = 5\n",
    "train_all_data(epoques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "55b8a86dd3afecd7a3419cfe1ac2f4ca1aa6e4714b8464444af28cca0c40e7fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
